{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Tool\n",
    "\n",
    "This is a set of tools that should help to get up to speed when delivering Visual Recognition projects. It provides helpers to simplify the training, testing and evaluation of classifiers.\n",
    "This particular tool helps you to automate blind set validation for IBM Watson Visual Recognition classifiers.\n",
    "\n",
    "## Features\n",
    "- Automated Classifier Testing\n",
    "- Persisting of test and result sets\n",
    "\n",
    "## Image Corpus Layout\n",
    "\n",
    "Currently the tooling is working with image corpora that are file and folder based. An image corpus can consist of several folders. Each folder represents a class the respective classifier will be able to recognize. Each class folder  contains all images that will be used to test the classifier on this class.\n",
    "\n",
    "To get a better understanding of the layout, take a look at this sample folder hierarchy (also contained in this project):\n",
    "\n",
    "```\n",
    " ./corpus\n",
    "     /mercedes_blindtest\n",
    "         /sclass\n",
    "             sclass_1.jpg\n",
    "             ...\n",
    "         /negative_examples\n",
    "             negative_sclass_1.jpg\n",
    "             ...\n",
    "```\n",
    "## Process\n",
    "1. Prepare your image set: Create a folder in the corpus directory that contains a subfolder for each class of your classifier your want to test. Each subfolder contains the images you want to use for testing.\n",
    "2. Make sure your config.ini file contains the right API key (either IAM or old API key)\n",
    "3. Set the classifier ID of the classifier you want to test.\n",
    "4. Run Tests\n",
    "5. Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import configparser\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import sklearn helpers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "from scipy import interp\n",
    "\n",
    "# import custom VR tooling libs\n",
    "import vrtool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "\n",
    "When using this tool for the first time, you'll find a file called **dummy.config.ini** which needs to be copied and renamed to **config.ini**.\n",
    "\n",
    "\n",
    "Configure *your* tool by entering your IAM API key and URL of the Visual Recognition service instance.\n",
    "```\n",
    "[vr]\n",
    "IAM_API_KEY:your_IAM_api_key\n",
    "URL:your_service_url\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Overview & Statistics\n",
    "\n",
    "The following section provides an extensive overview of the image corpus and statistics of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the folder that contains the corpora, currently relative to notebook location\n",
    "corpora_folder_name = '../corpus'\n",
    "config_name = 'config.ini'\n",
    "\n",
    "runner = vrtool.Runner(corpora_folder_name, config_name=config_name)\n",
    "corpora = runner.get_available_corpora()\n",
    "\n",
    "# Print a summary of the available corpora in the corpora directory\n",
    "print()\n",
    "print('Available image corpora:')\n",
    "print('\\n'.join('{}: {[0]}'.format(*el) for el in enumerate(corpora)))\n",
    "\n",
    "# Corpus Config\n",
    "corpus_to_test = 'mercedes_blindtest'\n",
    "\n",
    "# Statistics\n",
    "statistics = {}\n",
    "statistics['corpusname'] = corpus_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a detailed overview of the different classes and their distribution within each corpus\n",
    "corpora_info = runner.get_corpora_info(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [el['image_info'] for el in corpora_info if el['corpus_name'] == corpus_to_test ][0]\n",
    "negative_test = []\n",
    "test_data = test_data.groupby('class_name').filter(lambda x: len(x) >= 1)\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Classifier \n",
    "\n",
    "Performs classifier testing by packaging the image data into several zip files and sending them to the Visual Recognition Service for scoring. \n",
    "\n",
    "Main steps:\n",
    "1. Get the relevant classifier ids to be used for testing\n",
    "2. Perform the tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select classifier IDs to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(runner.vr_instance.list_classifiers().get_result(), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set classifier ID\n",
    "classifier_id = 'CLASSIFIER_ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Tests\n",
    "\n",
    "Test the classifier based on the experiments defined in the previous steps. This might take a couple of minutes depending on the number of images used for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(negative_test) >0):\n",
    "    test_data = pd.concat([test_data, negative_test])\n",
    "    \n",
    "# perform test\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "test_results = runner.test_classifier_with_data_frame(classifier_id, test_data)\n",
    "end = datetime.datetime.now()\n",
    "\n",
    "print(\"Testing finished after: \",end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_result = runner.vr_service.parse_img_results(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In this section the classifier performance is analyzed based on the tests that were performed in the previous steps.\n",
    "A confusion matrix is created to analyze the true & false / positives & negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load external data set for evaluation\n",
    "By deafult this cell does nothing and uses the data set that was created in this notebook.\n",
    "\n",
    "You can also use previously created experiment pickle files to test classifiers by setting the **USE_EXTERNAL_RESULT_DATA** to **True** and specify the path to the external experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If False, use result data from the current test run in this notebook\n",
    "USE_EXTERNAL_RESULT_DATA = False\n",
    "\n",
    "# Otherwise, external result data (filename.pkl) will be used from the specified path\n",
    "EXTERNAL_RESULT_PATH='modelconfigurations/YOUR_EVALUATION_FILE.pkl'\n",
    "\n",
    "if USE_EXTERNAL_RESULT_DATA:\n",
    "    with open(EXTERNAL_RESULT_PATH,'rb') as f:\n",
    "        evaluation = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_EXTERNAL_RESULT_DATA:\n",
    "    # match results against expected classification results\n",
    "    evaluation = runner.merge_predicted_and_target_labels(test_data, test_results)\n",
    "\n",
    "    # save evaluation results for further analysis and documentation\n",
    "    evaluation.to_pickle(\"modelconfigurations/\"+corpus_to_test + \"_result_\" +time.strftime(\"%d-%m-%Y-%H-%M-%S\")+ \".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Classification Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.evaluation_result_to_csv(evaluation, corpus_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot confusioin matrix as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "pd.options.display.max_colwidth = 600\n",
    "classification_reports = []\n",
    "confusion_matrices = []\n",
    "\n",
    "for threshold in thresholds: \n",
    "    ev = evaluation.copy()\n",
    "    ev.loc[ev['predicted_score_1'] < threshold,'predicted_class_1'] = 'None'\n",
    "    y_actual, y_pred = runner.get_y_values(ev)\n",
    "    confusion_matrix = pd.crosstab(y_actual, y_pred)\n",
    "    confusion_matrices.append((threshold, confusion_matrix))\n",
    "    print(\"Overall Accuracy for threshold {0}: {1}\".format(threshold ,metrics.accuracy_score(y_actual, y_pred)))\n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix)\n",
    "    classification_report = runner.get_classification_report(y_actual, y_pred)\n",
    "    classification_reports.append((threshold, classification_report))\n",
    "    print(\"\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report)\n",
    "    print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Classification Reports as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.classification_reports_to_csv(classification_reports, corpus_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Confusion Matrix as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.confusion_matrix_to_csv(confusion_matrices, corpus_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot confusioin matrix as chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract actual and predicted values from evaluation\n",
    "y_actual, y_pred = runner.get_y_values(evaluation)\n",
    "\n",
    "# plot confusion matrix\n",
    "confmatrix = runner.get_confusion_matrix(y_actual, y_pred)\n",
    "\n",
    "runner.plot_confusion_matrix(confmatrix, y_actual, y_pred, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Classification Report\n",
    "Creates a classification report including the most important metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "runner.print_classification_report(evaluation, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall Accuracy:\",metrics.accuracy_score(y_actual, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize False Positives & False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "threshold = 0.75 \n",
    "ev = evaluation.copy()\n",
    "ev.loc[ev['predicted_score_1'] < threshold,'predicted_class_1'] = 'None'\n",
    "\n",
    "# extract actual and predicted values from evaluation\n",
    "y_actual, y_pred = runner.get_y_values(ev)\n",
    "\n",
    "fpfn = ev[ y_actual!= y_pred ]\n",
    "\n",
    "image_count = fpfn.shape[0]\n",
    "\n",
    "fig = plt.figure(figsize=(40,30))\n",
    "\n",
    "columns = 5\n",
    "idx = 0\n",
    "\n",
    "for i, row in fpfn.iterrows():\n",
    "    image = mpimg.imread(row['image_x'])\n",
    "    ax = fig.add_subplot(int(image_count / columns + 1), columns, idx + 1)\n",
    "    ax.set_title(\"is: \"+row['class_name']\n",
    "                         +\"\\n pred: \"\n",
    "                         + row['predicted_class_1']\n",
    "                         +\" \\n file: \"\n",
    "                         +row['image_x'].split('/')[-1]\n",
    "                         +\" \\n score: \"\n",
    "                         +str(row['predicted_score_1']), fontsize=25)\n",
    "    idx = idx +1\n",
    "    ax.imshow(image, aspect='auto')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram Threshold Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_scores = evaluation['predicted_score_1']\n",
    "\n",
    "n, bins, patches = plt.hist(result_scores, 20, normed=0, facecolor='green', alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.zip_helper.clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
